Ensembles Trees Overview:
Comencemos por definir la palabra ensamble en el contexto de análisis de modelos como: un conjunto grande de modelos que se usan juntos como un “meta modelo”.
El método de ensamble busca incrementar la exactitud de las predicciones, a partir de la combinación de las predicciones de múltiples modelos como los arboles de decisión, con los cuales individualmente ya se obtienen salidas con alta precisión, al ser preparados con datos aleatorios. Debido a la aleatoriedad de cada modelo combinada se produce un incremento en la exactitud de las predicciones disminuyendo los errores.
Existen varias razones para el uso de métodos basados en ensamble, entre las que se cuentan: las estadísticas, grandes volúmenes de datos, existencia de muy escasos datos, principio de divide y vencerás y fusión de datos de diferentes fuentes.
Existen varios tipos de métodos de ensamble que se pueden implementar cuando se está llevando a cabo procesamiento con algoritmos de Machine Learning. Esos tipos de ensamble se pueden categorizar de diferentes maneras, pero podemos pensar básicamente en dos tipos de aproximaciones para la combinación de clasificadores: 
Una aproximación es usar clasificadores similares y combinarlos usando técnicas como bagging, boosting o random forests. Una segunda aproximación es combinar diferentes clasificadores usando model stacking. 
El método de Boosting es una técnica poderosa para combinar múltiples clasificadores de base para producir una forma de comité cuyo desempeño puede ser significativamente mejor que el de cada uno de los clasificadores originales. Puede dar buenos resultados aunque los clasificadores de base solo sean ligeramente mejores que el azar, por ello son conocidos como aprendices débiles (weak learners).
El método de Bagging (Agregación Bootstrap) es un método simple y efectivo para crear ensambles de modelos a partir de una familia inicial, reduce la varianza al promediar predictores diversos, usa bootstraps y predictores inestables, se suele usar con árboles y redes.
El método de Random Forets es una mejora de bagging sólo para árboles, realiza mejores predicciones que Bagging. Es muy usada al ser casi automático. Se obtienen resultados comparables a los mejores métodos actuales. 
En el método model stacking lo que se hace básicamente es construir varios modelos de diferentes tipos y luego combinamos las predicciones de esos modelos primarios para construir un modelo supervisor que aprende la mejor manera de combinar esas predicciones.
Por todo lo anterior, se puede concluir que los diferentes métodos de ensambles bajo sus algoritmos buscan obtener un mejor rendimiento predictivo frente al que se puede obtener por los algoritmos de modelos individuales.
